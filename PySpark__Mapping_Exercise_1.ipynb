{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saurater/ciencia_de_dados_pyspark/blob/main/PySpark__Mapping_Exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark - Mapping de Arquivo Texto - Parte I\n",
        "\n",
        "## Adaptação do IGTI por Sam Faraday\n",
        "\n",
        "Teste Inicial no Databricks\n",
        "June 2022"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c233a29b-65a1-4708-b383-3e6398c2688d"
        },
        "id": "V7rpmWN2CMje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "id": "p-Lyy7XEC7VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark  =  SparkSession.builder.appName(\"Mapeando_um_Arquivo\").getOrCreate()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Importa e Inicia uma Sessão",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "e3cbe37e-eb71-4154-9e36-b0da7be52ed5"
        },
        "id": "emDOhokjCMji"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "meu_arquivo_read_me = spark.sparkContext.textFile(\"SPARK_README.md\")"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Carrega o arquivo SPARK_READ_.MD - vc precisa fazer o upload antes no Databricks",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "003f2dd0-2c7c-46a2-94bf-a70618046cfe"
        },
        "id": "z9B_k-JUCMjk"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "#Usando uma função lambda, pega-se linha por linha e seu len (tamanho)\n",
        "mapa_do_meu_arquivo_read_me = meu_arquivo_read_me.map(lambda linha_do_texto : (linha_do_texto, len(linha_do_texto)))"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Usanda uma função lambda, para cada linha ele adiciona o len (cumprimento) da linha",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "ede42bbb-e102-4c1a-8f96-d52633245cf1"
        },
        "id": "U5eKJt3gCMjk"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# Nada será impresso em tela. Apenas nos workers, por isso as duas linhas alternativas seguintes\n",
        "mapa_do_meu_arquivo_read_me.foreach(print)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Atenção: Não será impresso nenhum resultado em tela, apenas nos workers.  As 2 linhas seguintes são uma alternativa",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "756a1617-be8b-453d-84f2-6a8728b99ecf"
        },
        "id": "DiAQTtd7CMjk"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# Pega-se os dados dos Workers e tra-se para o Master com a função collect\n",
        "saida_mapeada = mapa_do_meu_arquivo_read_me.collect()"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Trago os dados dos Workders para o Master, colocando na variável saida_mapeada",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "085cd7a7-384f-44a7-8a03-92532fdeef8b"
        },
        "id": "jcqp2ym4CMjl"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "#imprime linha a linha, mostrando o tamanho de cada uma\n",
        "for cada_linha  in saida_mapeada:\n",
        "        print(cada_linha)\n",
        "       "
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "Imprimo linha a linha da variável saida_mapeada",
          "showTitle": true,
          "inputWidgets": {},
          "nuid": "789af03b-42bd-4d18-9089-f73d51f2e613"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2PTKt-CMjl",
        "outputId": "8574d3ad-90ff-4aae-d64e-32a1ceb1d384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Apache Spark', 12)\n",
            "('', 0)\n",
            "('Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Scala, Java, Python, and R, and an optimized engine that supports general computation graphs for data analysis. It also supports a rich set of higher-level tools including Spark SQL for SQL and DataFrames, pandas API on Spark for pandas workloads, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for stream processing.', 452)\n",
            "('', 0)\n",
            "('Online Documentation', 20)\n",
            "('You can find the latest Spark documentation, including a programming guide, on the project web page. This README file only contains basic setup instructions.', 157)\n"
          ]
        }
      ],
      "execution_count": 10
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Mapping_Exercise_1",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 3279161451786732
    },
    "colab": {
      "name": "PySpark _Mapping_Exercise_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}